{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Ass3.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOCFG1FadALB5nvRP2w7oXt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mearoche/text-mining-tutorial/blob/master/Ass3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. Data import and preparation/clean up"
      ],
      "metadata": {
        "id": "LyNS9SNOZa6S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Set up"
      ],
      "metadata": {
        "id": "_Gk76RJ1Zf7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# For Colab: \n",
        "# 1. First CHANGE RUNTIME TYPE to GPU \n",
        "# 2. Then run install commands commented out below\n",
        "# 3. Then RESTART RUNTIME\n",
        "# 4. Then run git clone command commented out below\n",
        "# 5. Then run all the other cells"
      ],
      "metadata": {
        "id": "cBgkhw8FZjlw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages\n",
        "#!pip install --upgrade jax==0.2.3 jaxlib==0.1.56+cuda110 -f https://storage.googleapis.com/jax-releases/jax_releases.html\n",
        "#!pip install --upgrade numpyro==0.4.1\n",
        "#!pip install flashtext\n",
        "#!pip install contractions\n",
        "#!pip install lda\n",
        "#!pip install --upgrade spacy==2.2.4\n",
        "#!pip install --upgrade folium==0.2.1\n",
        "#!pip install topic-modelling-tools"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S7LmlVB9Zlyo",
        "outputId": "f88dbf39-2814-40e2-c1f1-2edf006b17cf"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in links: https://storage.googleapis.com/jax-releases/jax_releases.html\n",
            "Collecting jax==0.2.3\n",
            "  Downloading jax-0.2.3.tar.gz (473 kB)\n",
            "\u001b[K     |████████████████████████████████| 473 kB 4.2 MB/s \n",
            "\u001b[?25hCollecting jaxlib==0.1.56+cuda110\n",
            "  Downloading https://storage.googleapis.com/jax-releases/cuda110/jaxlib-0.1.56%2Bcuda110-cp37-none-manylinux2010_x86_64.whl (155.5 MB)\n",
            "\u001b[K     |████████████████████████████████| 155.5 MB 9.8 kB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.7/dist-packages (from jax==0.2.3) (1.21.5)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from jax==0.2.3) (1.0.0)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.7/dist-packages (from jax==0.2.3) (3.3.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from jaxlib==0.1.56+cuda110) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py->jax==0.2.3) (1.15.0)\n",
            "Building wheels for collected packages: jax\n",
            "  Building wheel for jax (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for jax: filename=jax-0.2.3-py3-none-any.whl size=542173 sha256=64d2c6fcfe129edf28c65b8e8e91cb35751b94130d8369c76530c571b5b0705f\n",
            "  Stored in directory: /root/.cache/pip/wheels/93/6f/2d/ee26ee4ada4c80f15c60eacb11a410d20f59e244bfea506111\n",
            "Successfully built jax\n",
            "Installing collected packages: jaxlib, jax\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.3.2+cuda11.cudnn805\n",
            "    Uninstalling jaxlib-0.3.2+cuda11.cudnn805:\n",
            "      Successfully uninstalled jaxlib-0.3.2+cuda11.cudnn805\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.3.4\n",
            "    Uninstalling jax-0.3.4:\n",
            "      Successfully uninstalled jax-0.3.4\n",
            "Successfully installed jax-0.2.3 jaxlib-0.1.56+cuda110\n",
            "Collecting numpyro==0.4.1\n",
            "  Downloading numpyro-0.4.1-py3-none-any.whl (176 kB)\n",
            "\u001b[K     |████████████████████████████████| 176 kB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from numpyro==0.4.1) (4.63.0)\n",
            "Requirement already satisfied: jaxlib==0.1.56 in /usr/local/lib/python3.7/dist-packages (from numpyro==0.4.1) (0.1.56+cuda110)\n",
            "Requirement already satisfied: jax==0.2.3 in /usr/local/lib/python3.7/dist-packages (from numpyro==0.4.1) (0.2.3)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.7/dist-packages (from jax==0.2.3->numpyro==0.4.1) (3.3.0)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.7/dist-packages (from jax==0.2.3->numpyro==0.4.1) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.12 in /usr/local/lib/python3.7/dist-packages (from jax==0.2.3->numpyro==0.4.1) (1.21.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from jaxlib==0.1.56->numpyro==0.4.1) (1.4.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py->jax==0.2.3->numpyro==0.4.1) (1.15.0)\n",
            "Installing collected packages: numpyro\n",
            "Successfully installed numpyro-0.4.1\n",
            "Collecting flashtext\n",
            "  Downloading flashtext-2.7.tar.gz (14 kB)\n",
            "Building wheels for collected packages: flashtext\n",
            "  Building wheel for flashtext (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for flashtext: filename=flashtext-2.7-py2.py3-none-any.whl size=9309 sha256=438a59e1b7bb1780f69b30ca5c5af49a16965762c5f4719f99d94e0e0735ea29\n",
            "  Stored in directory: /root/.cache/pip/wheels/cb/19/58/4e8fdd0009a7f89dbce3c18fff2e0d0fa201d5cdfd16f113b7\n",
            "Successfully built flashtext\n",
            "Installing collected packages: flashtext\n",
            "Successfully installed flashtext-2.7\n",
            "Collecting contractions\n",
            "  Downloading contractions-0.1.68-py2.py3-none-any.whl (8.1 kB)\n",
            "Collecting textsearch>=0.0.21\n",
            "  Downloading textsearch-0.0.21-py2.py3-none-any.whl (7.5 kB)\n",
            "Collecting anyascii\n",
            "  Downloading anyascii-0.3.1-py3-none-any.whl (287 kB)\n",
            "\u001b[K     |████████████████████████████████| 287 kB 3.9 MB/s \n",
            "\u001b[?25hCollecting pyahocorasick\n",
            "  Downloading pyahocorasick-1.4.4-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (106 kB)\n",
            "\u001b[K     |████████████████████████████████| 106 kB 55.9 MB/s \n",
            "\u001b[?25hInstalling collected packages: pyahocorasick, anyascii, textsearch, contractions\n",
            "Successfully installed anyascii-0.3.1 contractions-0.1.68 pyahocorasick-1.4.4 textsearch-0.0.21\n",
            "Collecting lda\n",
            "  Downloading lda-2.0.0-cp37-cp37m-manylinux1_x86_64.whl (351 kB)\n",
            "\u001b[K     |████████████████████████████████| 351 kB 3.9 MB/s \n",
            "\u001b[?25hCollecting pbr<4,>=0.6\n",
            "  Downloading pbr-3.1.1-py2.py3-none-any.whl (99 kB)\n",
            "\u001b[K     |████████████████████████████████| 99 kB 9.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from lda) (1.21.5)\n",
            "Installing collected packages: pbr, lda\n",
            "Successfully installed lda-2.0.0 pbr-3.1.1\n",
            "Requirement already satisfied: spacy==2.2.4 in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.4) (1.1.3)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.4) (0.9.0)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.4) (1.0.6)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.4) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.4) (2.0.6)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.4) (1.0.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.4) (3.0.6)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.4) (7.4.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.4) (1.21.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.4) (4.63.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.4) (57.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.4) (2.23.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy==2.2.4) (0.4.1)\n",
            "Requirement already satisfied: importlib-metadata>=0.20 in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy==2.2.4) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.2.4) (3.7.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spacy==2.2.4) (3.10.0.2)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.4) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.4) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.4) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy==2.2.4) (2021.10.8)\n",
            "Collecting folium==0.2.1\n",
            "  Downloading folium-0.2.1.tar.gz (69 kB)\n",
            "\u001b[K     |████████████████████████████████| 69 kB 3.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: Jinja2 in /usr/local/lib/python3.7/dist-packages (from folium==0.2.1) (2.11.3)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.7/dist-packages (from Jinja2->folium==0.2.1) (2.0.1)\n",
            "Building wheels for collected packages: folium\n",
            "  Building wheel for folium (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for folium: filename=folium-0.2.1-py3-none-any.whl size=79808 sha256=fb419ea125fb526c134cef35da613669ccde870bfb92be53fd8a4ec688d6f580\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/f0/3a/3f79a6914ff5affaf50cabad60c9f4d565283283c97f0bdccf\n",
            "Successfully built folium\n",
            "Installing collected packages: folium\n",
            "  Attempting uninstall: folium\n",
            "    Found existing installation: folium 0.8.3\n",
            "    Uninstalling folium-0.8.3:\n",
            "      Successfully uninstalled folium-0.8.3\n",
            "Successfully installed folium-0.2.1\n",
            "Collecting topic-modelling-tools\n",
            "  Downloading topic-modelling-tools-0.7.dev0.tar.gz (4.0 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.0 MB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.7/dist-packages (from topic-modelling-tools) (1.21.5)\n",
            "Requirement already satisfied: nltk>=3.2.4 in /usr/local/lib/python3.7/dist-packages (from topic-modelling-tools) (3.2.5)\n",
            "Requirement already satisfied: pandas>=0.20.3 in /usr/local/lib/python3.7/dist-packages (from topic-modelling-tools) (1.3.5)\n",
            "Requirement already satisfied: scipy>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from topic-modelling-tools) (1.4.1)\n",
            "Requirement already satisfied: Cython>=0.20.1 in /usr/local/lib/python3.7/dist-packages (from topic-modelling-tools) (0.29.28)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk>=3.2.4->topic-modelling-tools) (1.15.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.20.3->topic-modelling-tools) (2018.9)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.20.3->topic-modelling-tools) (2.8.2)\n",
            "Building wheels for collected packages: topic-modelling-tools\n",
            "  Building wheel for topic-modelling-tools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for topic-modelling-tools: filename=topic_modelling_tools-0.7.dev0-cp37-cp37m-linux_x86_64.whl size=329667 sha256=dcc5f4080dbea2bb8081a59ee41c232cffd8d81f354e4e34dba1871daf2c5018\n",
            "  Stored in directory: /root/.cache/pip/wheels/e9/d0/ee/e1a1f534a2794a356b473a713dc6905463aed85025499c6c2d\n",
            "Successfully built topic-modelling-tools\n",
            "Installing collected packages: topic-modelling-tools\n",
            "Successfully installed topic-modelling-tools-0.7.dev0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Restart RUNTIME after installing packages!"
      ],
      "metadata": {
        "id": "PWo9ez2BZplT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cloning GitHub (to do only once! Then comment it)\n",
        "#!git clone https://mearoche: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@github.com/mearoche/text-mining-tutorial.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3NVMfWCZqQR",
        "outputId": "cf6b9513-f86e-414d-c7e7-f2095671b0eb"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'text-mining-tutorial'...\n",
            "remote: Enumerating objects: 707, done.\u001b[K\n",
            "remote: Counting objects: 100% (24/24), done.\u001b[K\n",
            "remote: Compressing objects: 100% (24/24), done.\u001b[K\n",
            "remote: Total 707 (delta 13), reused 0 (delta 0), pack-reused 683\u001b[K\n",
            "Receiving objects: 100% (707/707), 13.88 MiB | 9.69 MiB/s, done.\n",
            "Resolving deltas: 100% (358/358), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#cd text-mining-tutorial"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BhB6r4lsZrvn",
        "outputId": "5e1856e0-df40-4462-cf90-b85381a5e088"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/text-mining-tutorial\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Check that the directory contains the growthdata.csv file\n",
        "!ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7VgcRs-ZusU",
        "outputId": "83ea57b5-1d86-4348-966b-df03ff83653a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ass3.ipynb  requirements.txt\t    tutorial_notebook.ipynb\n",
            "README.md   speech_data_extend.txt  tutorial.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install necessary packages\n",
        "\n",
        "# JAX\n",
        "import jax\n",
        "from jax import random, vmap, jit\n",
        "import jax.numpy as jnp\n",
        "import jax.nn as nn\n",
        "from jax.random import PRNGKey as Key\n",
        "\n",
        "# Panda\n",
        "import pandas as pd\n",
        "\n",
        "# Numpyro\n",
        "import numpyro\n",
        "numpyro.set_platform(\"cpu\")\n",
        "import numpyro.distributions as dist\n",
        "from numpyro.infer import MCMC, NUTS, log_likelihood\n",
        "import numpyro.infer.util \n",
        "from numpyro.primitives import deterministic\n",
        "from numpyro.handlers import condition, substitute, block\n",
        "\n",
        "# Matplot\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Spacy\n",
        "import spacy\n",
        "spacy.load('en_core_web_sm')\n",
        "\n",
        "# LDA\n",
        "import lda\n",
        "\n",
        "# Others\n",
        "import time\n",
        "import string\n",
        "import topicmodels\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import sys\n",
        "sys.path.append('../pymodules')"
      ],
      "metadata": {
        "id": "ivVCF7D3ZxdI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import data"
      ],
      "metadata": {
        "id": "7HS0EAxyaTZ6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import US Presidential State-of-the-Union addresses from 1945 onwards\n",
        "data = pd.read_table(\"speech_data_extend.txt\", encoding=\"utf-8\")\n",
        "data = data[data.year >= 1945]"
      ],
      "metadata": {
        "id": "2P4UpG0taVEN"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the number of documents in the dataset\n",
        "len(data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcEcHHzogfxj",
        "outputId": "6385fbdc-2be9-402c-9a8b-44d01039b1a3"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "10260"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1. LDA"
      ],
      "metadata": {
        "id": "yPXDOkhLjWz9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pre-processing"
      ],
      "metadata": {
        "id": "1awPe-dbjYqv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# I define docsobj, tokenized\n",
        "docsobj = topicmodels.RawDocs(data.speech, \"long\")"
      ],
      "metadata": {
        "id": "b_G48Ep9ja6-"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the stop words list, from http://snowball.tartarus.org/algorithms/english/stop.txt (long version)\n",
        "docsobj.stopwords"
      ],
      "metadata": {
        "id": "0VzS5gFsnafO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "862f3899-94e6-49c8-c2ea-70bf36536f2e"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'all',\n",
              " 'also',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'another',\n",
              " 'any',\n",
              " 'are',\n",
              " 'as',\n",
              " 'at',\n",
              " 'back',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'could',\n",
              " 'did',\n",
              " 'do',\n",
              " 'does',\n",
              " 'doing',\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'even',\n",
              " 'ever',\n",
              " 'every',\n",
              " 'few',\n",
              " 'first',\n",
              " 'five',\n",
              " 'for',\n",
              " 'four',\n",
              " 'from',\n",
              " 'further',\n",
              " 'get',\n",
              " 'go',\n",
              " 'goes',\n",
              " 'had',\n",
              " 'has',\n",
              " 'have',\n",
              " 'having',\n",
              " 'he',\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'high',\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'however',\n",
              " 'i',\n",
              " 'if',\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'it',\n",
              " 'its',\n",
              " 'itself',\n",
              " 'just',\n",
              " 'least',\n",
              " 'less',\n",
              " 'like',\n",
              " 'long',\n",
              " 'made',\n",
              " 'make',\n",
              " 'many',\n",
              " 'me',\n",
              " 'more',\n",
              " 'most',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'never',\n",
              " 'new',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'of',\n",
              " 'off',\n",
              " 'old',\n",
              " 'on',\n",
              " 'once',\n",
              " 'one',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'ought',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 'put',\n",
              " 'said',\n",
              " 'same',\n",
              " 'say',\n",
              " 'says',\n",
              " 'second',\n",
              " 'see',\n",
              " 'seen',\n",
              " 'she',\n",
              " 'should',\n",
              " 'since',\n",
              " 'so',\n",
              " 'some',\n",
              " 'still',\n",
              " 'such',\n",
              " 'take',\n",
              " 'than',\n",
              " 'that',\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " 'this',\n",
              " 'those',\n",
              " 'three',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'two',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 'us',\n",
              " 'very',\n",
              " 'was',\n",
              " 'way',\n",
              " 'we',\n",
              " 'well',\n",
              " 'were',\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'whether',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'with',\n",
              " 'would',\n",
              " 'you',\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves'}"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Since we want to focus on words and not tokens, I will clean the tokens (all non-alphabetic and numeric tokens), and remove those with length less than 1\n",
        "docsobj.token_clean(1)"
      ],
      "metadata": {
        "id": "nSzOw-AQnxRk"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I remove the stopwords from docsobj.tokens\n",
        "docsobj.stopword_remove(\"tokens\")"
      ],
      "metadata": {
        "id": "5FISLTBtoYVX"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I want to group together words that are grammatically different but thematically identical, i.e. stemming, using Porter stemmer\n",
        "docsobj.stem()\n",
        "docsobj.stopword_remove(\"stems\") # I again remove stopwords as stemmed forms of tokens may be in the stopword list"
      ],
      "metadata": {
        "id": "qsBln3ewonUR"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use of TF-IDF on each stem to measure informativeness (to identify common words and rare words)\n",
        "docsobj.term_rank(\"stems\") # Outcome is 2 csv files, df_ranking.csv ranks each stem according to its document frequency, and tfidf_ranking.csv ranks each stem according to the tf-idf measure."
      ],
      "metadata": {
        "id": "D1d55d8mpI7F"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Plotting the ranking of the TF-IDF to explore a potential cutoff value\n",
        "plt.plot([x[1] for x in docsobj.tfidf_ranking])"
      ],
      "metadata": {
        "id": "MYC_lqaEpvPj",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "outputId": "d160350a-2fe8-431a-a6c4-9b5e4f00e378"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fba8f52e590>]"
            ]
          },
          "metadata": {},
          "execution_count": 49
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "I take as cutoff 5,000 as it seems reasonable according to the above plot."
      ],
      "metadata": {
        "id": "7mAEIPF8qkyA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# I now drop the stems below this cutoff\n",
        "cutoff = 5000\n",
        "docsobj.rank_remove(\"tfidf\",\"stems\",docsobj.tfidf_ranking[cutoff][1])\n",
        "all_stems = [s for d in docsobj.stems for s in d]"
      ],
      "metadata": {
        "id": "nqh6Fyauqryp"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check number of unique stems\n",
        "print(len(set(all_stems)))\n",
        "V = len(set(all_stems))\n",
        "# Check number of total stems\n",
        "print(len(all_stems))"
      ],
      "metadata": {
        "id": "Fm9QMEbarCSH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1518c669-4cb0-4b38-a2e2-e68c360f6500"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4918\n",
            "268763\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, I obtain 4,918 unique stems and 268,763 total stems. I can now estimate my LDA."
      ],
      "metadata": {
        "id": "6o0cXUKmrO0S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Estimations"
      ],
      "metadata": {
        "id": "xF8hrugGpNUc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "I will estimate an LDA on individual paragraphs using the collapsed Gibbs sampling algorithm of Griffiths and Steyvers (2004)."
      ],
      "metadata": {
        "id": "L3QakVDVUQXg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# I choose the number of topics\n",
        "topics = 20\n",
        "ldaobj = topicmodels.LDA.LDAGibbs(docsobj.stems,topics)"
      ],
      "metadata": {
        "id": "03LSAanpUaqa"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# I choose the hyperparameters of the Dirichlet priors, here I follow Griffiths and Steyvers (2004)\n",
        "print(ldaobj.K) # number of topic, user defined, here topics = K = 20\n",
        "print(ldaobj.alpha) # hyperparameter for document-topic distribution, automatically defined as 50/K, with K the number of topics\n",
        "print(50/topics) # all good\n",
        "print(ldaobj.beta) # hyperparameter for topics, automatically defined as 200/V, with V the number of unique vocabulary elements\n",
        "print(200/V) # all good"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SbNY-p-2UuWR",
        "outputId": "1b25f798-e6df-4844-8148-18d4a34b6096"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "20\n",
            "2.5\n",
            "2.5\n",
            "0.040666937779585195\n",
            "0.040666937779585195\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, I will sample.\n",
        "\n",
        "To do so, I have to decide on three parameters:\n",
        "\n",
        "\n",
        "1.   Number of iterations we want the chain to burn in before beginning to sample (A)\n",
        "2.   Thinning interval: the number of iterations to let the chain run between samples (B)\n",
        "3.   Number of samples to take (C)\n",
        "\n",
        "Total number of iterations = A + B x C\n",
        "\n"
      ],
      "metadata": {
        "id": "3TWjTuErXLYb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# I select arbitrary values for A, B, and C keeping in mind that I do not want the code to take too long to run\n",
        "A = 0 \n",
        "B = 50\n",
        "C = 10\n",
        "ldaobj.sample(A,B,C) # Here 500 iterations\n",
        "ldaobj.perplexity() # To check goodness-of-fit of each of the C samples (the lower the value the better the fit)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_ttHEcrWi75",
        "outputId": "1f658aac-2c7c-4488-8f88-99ac785cb1a0"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 10 of (collapsed) Gibbs sampling\n",
            "Iteration 20 of (collapsed) Gibbs sampling\n",
            "Iteration 30 of (collapsed) Gibbs sampling\n",
            "Iteration 40 of (collapsed) Gibbs sampling\n",
            "Iteration 50 of (collapsed) Gibbs sampling\n",
            "Iteration 60 of (collapsed) Gibbs sampling\n",
            "Iteration 70 of (collapsed) Gibbs sampling\n",
            "Iteration 80 of (collapsed) Gibbs sampling\n",
            "Iteration 90 of (collapsed) Gibbs sampling\n",
            "Iteration 100 of (collapsed) Gibbs sampling\n",
            "Iteration 110 of (collapsed) Gibbs sampling\n",
            "Iteration 120 of (collapsed) Gibbs sampling\n",
            "Iteration 130 of (collapsed) Gibbs sampling\n",
            "Iteration 140 of (collapsed) Gibbs sampling\n",
            "Iteration 150 of (collapsed) Gibbs sampling\n",
            "Iteration 160 of (collapsed) Gibbs sampling\n",
            "Iteration 170 of (collapsed) Gibbs sampling\n",
            "Iteration 180 of (collapsed) Gibbs sampling\n",
            "Iteration 190 of (collapsed) Gibbs sampling\n",
            "Iteration 200 of (collapsed) Gibbs sampling\n",
            "Iteration 210 of (collapsed) Gibbs sampling\n",
            "Iteration 220 of (collapsed) Gibbs sampling\n",
            "Iteration 230 of (collapsed) Gibbs sampling\n",
            "Iteration 240 of (collapsed) Gibbs sampling\n",
            "Iteration 250 of (collapsed) Gibbs sampling\n",
            "Iteration 260 of (collapsed) Gibbs sampling\n",
            "Iteration 270 of (collapsed) Gibbs sampling\n",
            "Iteration 280 of (collapsed) Gibbs sampling\n",
            "Iteration 290 of (collapsed) Gibbs sampling\n",
            "Iteration 300 of (collapsed) Gibbs sampling\n",
            "Iteration 310 of (collapsed) Gibbs sampling\n",
            "Iteration 320 of (collapsed) Gibbs sampling\n",
            "Iteration 330 of (collapsed) Gibbs sampling\n",
            "Iteration 340 of (collapsed) Gibbs sampling\n",
            "Iteration 350 of (collapsed) Gibbs sampling\n",
            "Iteration 360 of (collapsed) Gibbs sampling\n",
            "Iteration 370 of (collapsed) Gibbs sampling\n",
            "Iteration 380 of (collapsed) Gibbs sampling\n",
            "Iteration 390 of (collapsed) Gibbs sampling\n",
            "Iteration 400 of (collapsed) Gibbs sampling\n",
            "Iteration 410 of (collapsed) Gibbs sampling\n",
            "Iteration 420 of (collapsed) Gibbs sampling\n",
            "Iteration 430 of (collapsed) Gibbs sampling\n",
            "Iteration 440 of (collapsed) Gibbs sampling\n",
            "Iteration 450 of (collapsed) Gibbs sampling\n",
            "Iteration 460 of (collapsed) Gibbs sampling\n",
            "Iteration 470 of (collapsed) Gibbs sampling\n",
            "Iteration 480 of (collapsed) Gibbs sampling\n",
            "Iteration 490 of (collapsed) Gibbs sampling\n",
            "Iteration 500 of (collapsed) Gibbs sampling\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1011.09743298,  970.80919847,  955.11161422,  948.25245364,\n",
              "        944.53823302,  941.56609815,  940.73081949,  939.6240015 ,\n",
              "        937.65712693,  937.67412674])"
            ]
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TO WORK ON HERE:\n",
        "Here I need to use a convergence criterion to decide how many iterations I should use based on a minimized perplexity (like I could say to Python to stop when perplexity does not improve by more than 10^-2...)"
      ],
      "metadata": {
        "id": "UnNwhBdeY6zG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Keeping the last n samples (as they are the ones with the lowest perplexity, n is arbitrary)\n",
        "n = 4 # I SHOULD NORMALLY TAKE AS MANY AS COMPUTATIONALLY FEASIBLE\n",
        "ldaobj.samples_keep(n)"
      ],
      "metadata": {
        "id": "n9E3z2BDYoZH"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Let us look at the shape:\n",
        "    # of my estimated topics\n",
        "print(ldaobj.tt.shape)\n",
        "    # of my estimated document-topic distributions\n",
        "print(ldaobj.dt.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vm0XEp2sZc7m",
        "outputId": "cb4b9d12-5103-4ded-c06f-c930c449b9e7"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4918, 20, 4)\n",
            "(10260, 20, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Now I will extract a csv file containing the first m stems in each topic ranked according their probability, using the final stored sample\n",
        "m = 20\n",
        "ldaobj.topic_content(m)"
      ],
      "metadata": {
        "id": "bwS5r0hyaUjv"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I have checked the topics and they seem reasonable, for example:\n",
        "\n",
        "- Topic 0: govern, feder, state, respons, local, public\n",
        "\n",
        "- Topic 6: world, peac, free, freedom, secur, america, fight, threat, danger\n",
        "\n",
        "- Topic 8: health, system, care, secur, need, social, servic\n",
        "\n",
        "- Topic 16: war, forc, defens, militari, soviet, power, nuclear, arm\n",
        "\n",
        "THESE MAY CHANGE WHEN OPTIMIZING THE ITERATIVE STEP ABOVE TO HAVE MORE SAMPLES AND A BETTER PERPLEXITY"
      ],
      "metadata": {
        "id": "_D3uA9xOahax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2. Ranking topics according to whether they are more D or R"
      ],
      "metadata": {
        "id": "nh0R6TnjLxy4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One way to do so would be to use the respective partisanship of each president in the sample: Truman (D), Eisenhower (R), Kennedy (D), Johnson (D), Nixon (R), Ford (R), Carter (D), Reagan (R), Bush Senior (R), Clinton (D), Bush Jr (R), and Obama (D). \n",
        "\n",
        "Using this information and linking it with the topics obtained will allow us to rank them as either Democrat (D) or Republican (R). A general rule to use could be the majority, i.e., if a topic is used more than 50% of the time by a given party, it is assigned to that party.\n",
        "\n",
        "I also need to account for the fact that there maybe more years with R presidents than D presidents..."
      ],
      "metadata": {
        "id": "p0AWB5hbL-zR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Estimated distribution of the m topics (m columns) within each document (each row)\n",
        "dt = ldaobj.dt_avg() # suming each row = 1\n",
        "\n",
        "## This kind of gives in each cell the proportion that a topic represents in each document"
      ],
      "metadata": {
        "id": "hXxGyoPGQlWf"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Estimated distribution of the m topics (m columns) within each stems (each row)\n",
        "tt = ldaobj.tt_avg() # suming each column = 1\n",
        "ldaobj.dict_print()\n",
        "\n",
        "## This kind of gives in each cell the associative power of a stem to a given topic"
      ],
      "metadata": {
        "id": "Lt7_uS8QRW0m"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let me now bring the analysis from the paragraph level to the speech level (one speech per year).\n",
        "\n",
        "I estimate the speech-level distribution using querying."
      ],
      "metadata": {
        "id": "Si_SLmx0YIMg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data['speech'] = [' '.join(s) for s in docsobj.stems] # I replace the speech field in the original data with its cleaned version from docsobj (after the processing done previously)\n",
        "aggspeeches = data.groupby(['year','president'])['speech'].apply(lambda x: ' '.join(x)) # I aggregate up to the speech level\n",
        "aggdocs = topicmodels.RawDocs(aggspeeches) # create new RawDocs object that contains entire speech stems in aggdocs.tokens\n",
        "queryobj = topicmodels.LDA.QueryGibbs(aggdocs.tokens,ldaobj.token_key,ldaobj.tt) # initialize query object with ldaobj attributes\n",
        "queryobj.query(10) # I query the selected samples (querying does not require a lot of iterations to obtain good perplexity)\n",
        "dt_query = queryobj.dt_avg()\n",
        "aggdata = pd.DataFrame(dt_query,index=aggspeeches.index,columns=['T' + str(i) for i in range(queryobj.K)]) # I aggregate\n",
        "aggdata.to_csv(\"final_output_agg.csv\") # Obtain final dataset"
      ],
      "metadata": {
        "id": "aWp26iNWWmq_"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I know have in \"final_output_agg.cv\" a row per speech and a column per topic. For each speech the row sums to 1 across all topics.\n"
      ],
      "metadata": {
        "id": "95Enmfl-ZNh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify the proportion of Democrat and Republican speechs in the sample\n",
        "final_data = pd.read_csv(\"final_output_agg.csv\")"
      ],
      "metadata": {
        "id": "3siDecqRa85L"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final_data # Check the data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "bRoWvOv8cMY0",
        "outputId": "044fa2bd-9762-47ed-aa10-d5f3f5fae412"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    year    president        T0        T1        T2        T3        T4  \\\n",
              "0   1945  RooseveltII  0.085622  0.029174  0.027956  0.015196  0.034580   \n",
              "1   1946       Truman  0.026391  0.071717  0.027040  0.052485  0.030861   \n",
              "2   1947       Truman  0.044272  0.040470  0.039878  0.036583  0.043849   \n",
              "3   1948       Truman  0.055982  0.066289  0.037591  0.049111  0.040825   \n",
              "4   1949       Truman  0.042162  0.062945  0.041122  0.027761  0.057749   \n",
              "..   ...          ...       ...       ...       ...       ...       ...   \n",
              "68  2010        Obama  0.037631  0.106647  0.037631  0.059068  0.074645   \n",
              "69  2011        Obama  0.052107  0.078038  0.070135  0.049473  0.055976   \n",
              "70  2012        Obama  0.047848  0.132317  0.070223  0.039950  0.061861   \n",
              "71  2013        Obama  0.043471  0.100031  0.053366  0.052976  0.058196   \n",
              "72  2014        Obama  0.061578  0.089807  0.055029  0.041855  0.065266   \n",
              "\n",
              "          T5        T6        T7  ...       T10       T11       T12       T13  \\\n",
              "0   0.015975  0.036431  0.046902  ...  0.080508  0.046074  0.057958  0.151666   \n",
              "1   0.069306  0.029266  0.182177  ...  0.047496  0.034588  0.020326  0.065894   \n",
              "2   0.054664  0.096654  0.053565  ...  0.044863  0.043089  0.043849  0.076969   \n",
              "3   0.041027  0.045574  0.056690  ...  0.043553  0.045574  0.034256  0.033852   \n",
              "4   0.052999  0.048100  0.057601  ...  0.028652  0.051069  0.031027  0.034145   \n",
              "..       ...       ...       ...  ...       ...       ...       ...       ...   \n",
              "68  0.021977  0.069864  0.038325  ...  0.031231  0.052128  0.139343  0.019201   \n",
              "69  0.024778  0.040747  0.025930  ...  0.031857  0.056882  0.121337  0.015476   \n",
              "70  0.032131  0.053809  0.021524  ...  0.028182  0.056209  0.113503  0.020749   \n",
              "71  0.018074  0.070661  0.026410  ...  0.030617  0.067389  0.090916  0.027734   \n",
              "72  0.021831  0.047501  0.029810  ...  0.032596  0.063385  0.099518  0.027176   \n",
              "\n",
              "         T14       T15       T16       T17       T18       T19  \n",
              "0   0.075638  0.058981  0.040425  0.041642  0.064095  0.028687  \n",
              "1   0.045605  0.114336  0.022626  0.019733  0.015783  0.042934  \n",
              "2   0.067252  0.091585  0.031599  0.021206  0.024248  0.040723  \n",
              "3   0.102971  0.090744  0.051940  0.048808  0.024353  0.069119  \n",
              "4   0.073337  0.128414  0.044685  0.040083  0.033254  0.057601  \n",
              "..       ...       ...       ...       ...       ...       ...  \n",
              "68  0.021437  0.022131  0.032927  0.074568  0.065932  0.011721  \n",
              "69  0.026589  0.017204  0.023955  0.076556  0.078943  0.023790  \n",
              "70  0.015872  0.020440  0.024466  0.066816  0.068520  0.015640  \n",
              "71  0.027189  0.017685  0.043549  0.052664  0.074400  0.014724  \n",
              "72  0.018594  0.017690  0.032445  0.057814  0.091915  0.012195  \n",
              "\n",
              "[73 rows x 22 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0b52ce1b-5f56-4fdf-9123-eaf4a12d7950\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>year</th>\n",
              "      <th>president</th>\n",
              "      <th>T0</th>\n",
              "      <th>T1</th>\n",
              "      <th>T2</th>\n",
              "      <th>T3</th>\n",
              "      <th>T4</th>\n",
              "      <th>T5</th>\n",
              "      <th>T6</th>\n",
              "      <th>T7</th>\n",
              "      <th>...</th>\n",
              "      <th>T10</th>\n",
              "      <th>T11</th>\n",
              "      <th>T12</th>\n",
              "      <th>T13</th>\n",
              "      <th>T14</th>\n",
              "      <th>T15</th>\n",
              "      <th>T16</th>\n",
              "      <th>T17</th>\n",
              "      <th>T18</th>\n",
              "      <th>T19</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1945</td>\n",
              "      <td>RooseveltII</td>\n",
              "      <td>0.085622</td>\n",
              "      <td>0.029174</td>\n",
              "      <td>0.027956</td>\n",
              "      <td>0.015196</td>\n",
              "      <td>0.034580</td>\n",
              "      <td>0.015975</td>\n",
              "      <td>0.036431</td>\n",
              "      <td>0.046902</td>\n",
              "      <td>...</td>\n",
              "      <td>0.080508</td>\n",
              "      <td>0.046074</td>\n",
              "      <td>0.057958</td>\n",
              "      <td>0.151666</td>\n",
              "      <td>0.075638</td>\n",
              "      <td>0.058981</td>\n",
              "      <td>0.040425</td>\n",
              "      <td>0.041642</td>\n",
              "      <td>0.064095</td>\n",
              "      <td>0.028687</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1946</td>\n",
              "      <td>Truman</td>\n",
              "      <td>0.026391</td>\n",
              "      <td>0.071717</td>\n",
              "      <td>0.027040</td>\n",
              "      <td>0.052485</td>\n",
              "      <td>0.030861</td>\n",
              "      <td>0.069306</td>\n",
              "      <td>0.029266</td>\n",
              "      <td>0.182177</td>\n",
              "      <td>...</td>\n",
              "      <td>0.047496</td>\n",
              "      <td>0.034588</td>\n",
              "      <td>0.020326</td>\n",
              "      <td>0.065894</td>\n",
              "      <td>0.045605</td>\n",
              "      <td>0.114336</td>\n",
              "      <td>0.022626</td>\n",
              "      <td>0.019733</td>\n",
              "      <td>0.015783</td>\n",
              "      <td>0.042934</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1947</td>\n",
              "      <td>Truman</td>\n",
              "      <td>0.044272</td>\n",
              "      <td>0.040470</td>\n",
              "      <td>0.039878</td>\n",
              "      <td>0.036583</td>\n",
              "      <td>0.043849</td>\n",
              "      <td>0.054664</td>\n",
              "      <td>0.096654</td>\n",
              "      <td>0.053565</td>\n",
              "      <td>...</td>\n",
              "      <td>0.044863</td>\n",
              "      <td>0.043089</td>\n",
              "      <td>0.043849</td>\n",
              "      <td>0.076969</td>\n",
              "      <td>0.067252</td>\n",
              "      <td>0.091585</td>\n",
              "      <td>0.031599</td>\n",
              "      <td>0.021206</td>\n",
              "      <td>0.024248</td>\n",
              "      <td>0.040723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1948</td>\n",
              "      <td>Truman</td>\n",
              "      <td>0.055982</td>\n",
              "      <td>0.066289</td>\n",
              "      <td>0.037591</td>\n",
              "      <td>0.049111</td>\n",
              "      <td>0.040825</td>\n",
              "      <td>0.041027</td>\n",
              "      <td>0.045574</td>\n",
              "      <td>0.056690</td>\n",
              "      <td>...</td>\n",
              "      <td>0.043553</td>\n",
              "      <td>0.045574</td>\n",
              "      <td>0.034256</td>\n",
              "      <td>0.033852</td>\n",
              "      <td>0.102971</td>\n",
              "      <td>0.090744</td>\n",
              "      <td>0.051940</td>\n",
              "      <td>0.048808</td>\n",
              "      <td>0.024353</td>\n",
              "      <td>0.069119</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1949</td>\n",
              "      <td>Truman</td>\n",
              "      <td>0.042162</td>\n",
              "      <td>0.062945</td>\n",
              "      <td>0.041122</td>\n",
              "      <td>0.027761</td>\n",
              "      <td>0.057749</td>\n",
              "      <td>0.052999</td>\n",
              "      <td>0.048100</td>\n",
              "      <td>0.057601</td>\n",
              "      <td>...</td>\n",
              "      <td>0.028652</td>\n",
              "      <td>0.051069</td>\n",
              "      <td>0.031027</td>\n",
              "      <td>0.034145</td>\n",
              "      <td>0.073337</td>\n",
              "      <td>0.128414</td>\n",
              "      <td>0.044685</td>\n",
              "      <td>0.040083</td>\n",
              "      <td>0.033254</td>\n",
              "      <td>0.057601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>68</th>\n",
              "      <td>2010</td>\n",
              "      <td>Obama</td>\n",
              "      <td>0.037631</td>\n",
              "      <td>0.106647</td>\n",
              "      <td>0.037631</td>\n",
              "      <td>0.059068</td>\n",
              "      <td>0.074645</td>\n",
              "      <td>0.021977</td>\n",
              "      <td>0.069864</td>\n",
              "      <td>0.038325</td>\n",
              "      <td>...</td>\n",
              "      <td>0.031231</td>\n",
              "      <td>0.052128</td>\n",
              "      <td>0.139343</td>\n",
              "      <td>0.019201</td>\n",
              "      <td>0.021437</td>\n",
              "      <td>0.022131</td>\n",
              "      <td>0.032927</td>\n",
              "      <td>0.074568</td>\n",
              "      <td>0.065932</td>\n",
              "      <td>0.011721</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>2011</td>\n",
              "      <td>Obama</td>\n",
              "      <td>0.052107</td>\n",
              "      <td>0.078038</td>\n",
              "      <td>0.070135</td>\n",
              "      <td>0.049473</td>\n",
              "      <td>0.055976</td>\n",
              "      <td>0.024778</td>\n",
              "      <td>0.040747</td>\n",
              "      <td>0.025930</td>\n",
              "      <td>...</td>\n",
              "      <td>0.031857</td>\n",
              "      <td>0.056882</td>\n",
              "      <td>0.121337</td>\n",
              "      <td>0.015476</td>\n",
              "      <td>0.026589</td>\n",
              "      <td>0.017204</td>\n",
              "      <td>0.023955</td>\n",
              "      <td>0.076556</td>\n",
              "      <td>0.078943</td>\n",
              "      <td>0.023790</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>70</th>\n",
              "      <td>2012</td>\n",
              "      <td>Obama</td>\n",
              "      <td>0.047848</td>\n",
              "      <td>0.132317</td>\n",
              "      <td>0.070223</td>\n",
              "      <td>0.039950</td>\n",
              "      <td>0.061861</td>\n",
              "      <td>0.032131</td>\n",
              "      <td>0.053809</td>\n",
              "      <td>0.021524</td>\n",
              "      <td>...</td>\n",
              "      <td>0.028182</td>\n",
              "      <td>0.056209</td>\n",
              "      <td>0.113503</td>\n",
              "      <td>0.020749</td>\n",
              "      <td>0.015872</td>\n",
              "      <td>0.020440</td>\n",
              "      <td>0.024466</td>\n",
              "      <td>0.066816</td>\n",
              "      <td>0.068520</td>\n",
              "      <td>0.015640</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>71</th>\n",
              "      <td>2013</td>\n",
              "      <td>Obama</td>\n",
              "      <td>0.043471</td>\n",
              "      <td>0.100031</td>\n",
              "      <td>0.053366</td>\n",
              "      <td>0.052976</td>\n",
              "      <td>0.058196</td>\n",
              "      <td>0.018074</td>\n",
              "      <td>0.070661</td>\n",
              "      <td>0.026410</td>\n",
              "      <td>...</td>\n",
              "      <td>0.030617</td>\n",
              "      <td>0.067389</td>\n",
              "      <td>0.090916</td>\n",
              "      <td>0.027734</td>\n",
              "      <td>0.027189</td>\n",
              "      <td>0.017685</td>\n",
              "      <td>0.043549</td>\n",
              "      <td>0.052664</td>\n",
              "      <td>0.074400</td>\n",
              "      <td>0.014724</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>72</th>\n",
              "      <td>2014</td>\n",
              "      <td>Obama</td>\n",
              "      <td>0.061578</td>\n",
              "      <td>0.089807</td>\n",
              "      <td>0.055029</td>\n",
              "      <td>0.041855</td>\n",
              "      <td>0.065266</td>\n",
              "      <td>0.021831</td>\n",
              "      <td>0.047501</td>\n",
              "      <td>0.029810</td>\n",
              "      <td>...</td>\n",
              "      <td>0.032596</td>\n",
              "      <td>0.063385</td>\n",
              "      <td>0.099518</td>\n",
              "      <td>0.027176</td>\n",
              "      <td>0.018594</td>\n",
              "      <td>0.017690</td>\n",
              "      <td>0.032445</td>\n",
              "      <td>0.057814</td>\n",
              "      <td>0.091915</td>\n",
              "      <td>0.012195</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>73 rows × 22 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0b52ce1b-5f56-4fdf-9123-eaf4a12d7950')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-0b52ce1b-5f56-4fdf-9123-eaf4a12d7950 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-0b52ce1b-5f56-4fdf-9123-eaf4a12d7950');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a dummy variable for Republican = 1, Democrat = 0\n",
        "\n",
        "### THE REST I AM CODING FOR NOW ON STATA BECAUSE I SUCK AT PYTHON. I WILL NEED THEN TO ASK SOMEONE TO HELP ME PUT IT BACK INTO PYTHON"
      ],
      "metadata": {
        "id": "H8wII2_rcYAt"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Ranking presidents on the basis of D-R scale"
      ],
      "metadata": {
        "id": "CNcu1vInN38w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One way to do so would be to define a scale as below:\n",
        "\n",
        "Most Democrat ----------------------------------------- Most Republican\n",
        "\n",
        "0\\% R topics ---------------------------------------------- 100\\% R topics\n",
        "\n",
        "100\\% D topics ---------------------------------------------- 0\\% D topics\n",
        "\n",
        "-1 rating ---------------------------------------------------- +1 rating\n",
        "\n",
        "The percentages would represent the average (across all speeches given) of the proportion of R or D topics used in each speech."
      ],
      "metadata": {
        "id": "ogXsINrLN96f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### THE REST WAS DONE IN STATA..."
      ],
      "metadata": {
        "id": "_53Q9V_hpOO_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}